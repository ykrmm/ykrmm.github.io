---
title: "[TMLR] ITEM: Improving Training and Evaluation of Message-Passing based GNNs for top-k recommendation"
collection: publications
permalink: /publication/2024-07-01_item
excerpt: 'First paper of the PhD. I worked on the improvement of Message-Passing based GNN for the task of top-k recommandation. Our contribution is to use a loss that better align the item ranking of a user'
date: 2024-07-01
venue: 'Transaction on Machine Learning Research'
paperurl: 'https://arxiv.org/abs/2407.07912'
citation: '@article{
karmim2024item,
title={{ITEM}: Improving Training and Evaluation of Message-Passing based {GNN}s for top-k recommendation},
author={Yannis Karmim and Elias Ramzi and Raphael Fournier-Sniehotta and Nicolas THOME},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=9B6LM2uoEs},
note={}}
'
---
Graph Neural Networks (GNNs), especially message-passing-based models, have become
prominent in top-k recommendation tasks, outperforming matrix factorization models due to
their ability to efficiently aggregate information from a broader context. Although GNNs are
evaluated with ranking-based metrics, e.g. NDCG@k and Recall@k, they remain largely trained
with proxy losses, e.g. the BPR loss. In this work we explore the use of ranking loss functions
to directly optimize the evaluation metrics, an area not extensively investigated in the GNN
community for collaborative filtering. We take advantage of smooth approximations of the rank
to facilitate end-to-end training of GNNs and propose a Personalized PageRank-based negative
sampling strategy tailored for ranking loss functions. Moreover, we extend the evaluation of GNN
models for top-k recommendation tasks with an inductive user-centric protocol, providing a more
accurate reflection of real-world applications. Our proposed method significantly outperforms
the standard BPR loss and more advanced losses across four datasets and four recent GNN
architectures while also exhibiting faster training. Demonstrating the potential of ranking loss
functions in improving GNN training for collaborative filtering tasks

[Download paper here](https://arxiv.org/abs/2407.07912)

